#!/usr/bin/env python
"""
ëª¨ë“  submission íŒŒì¼ì„ í‰ê°€í•˜ê³  ê°€ì¥ ë†’ì€ ìŠ¤ì½”ì–´ë¥¼ ê°€ì§„ íŒŒì¼ì„ ì°¾ëŠ” ìŠ¤í¬ë¦½íŠ¸
"""

import sys
from pathlib import Path

import pandas as pd
from sklearn.metrics import f1_score, roc_auc_score

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).resolve().parent.parent.parent
sys.path.insert(0, str(project_root))

# ê²½ë¡œ ì„¤ì •
base_dir = project_root
submissions_dir = base_dir / "submissions"
y_test_path = base_dir / "data" / "raw" / "Y_test.csv"

# Y_test ë¡œë“œ
y_test = pd.read_csv(y_test_path)
y_test = y_test.sort_values("ID").reset_index(drop=True)
y_true = y_test["HE_D3_label"].values

# ëª¨ë“  submission íŒŒì¼ ì°¾ê¸°
submission_files = list(submissions_dir.glob("submission*.csv"))
submission_files = [f for f in submission_files if f.name != "README.md"]

results = []

print("=" * 80)
print("ëª¨ë“  Submission íŒŒì¼ í‰ê°€ ì¤‘...")
print("=" * 80)

for submission_path in sorted(submission_files):
    try:
        # Submission íŒŒì¼ ë¡œë“œ
        submission = pd.read_csv(submission_path)
        submission = submission.sort_values("ID").reset_index(drop=True)

        # ID ì¼ì¹˜ í™•ì¸
        if not (submission["ID"] == y_test["ID"]).all():
            print(
                f"ê²½ê³ : {submission_path.name}ì˜ IDê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë³‘í•©í•©ë‹ˆë‹¤."
            )
            merged = pd.merge(
                y_test, submission, on="ID", suffixes=("_true", "_pred")
            )
            y_pred_prob = merged["HE_D3_label_pred"].values
        else:
            y_pred_prob = submission["HE_D3_label"].values

        # ì˜ˆì¸¡ê°’ìœ¼ë¡œ ë³€í™˜ (í™•ë¥  >= 0.5 -> 1, else -> 0)
        y_pred = (y_pred_prob >= 0.5).astype(int)

        # ë©”íŠ¸ë¦­ ê³„ì‚°
        f1 = f1_score(y_true, y_pred)
        auroc = roc_auc_score(y_true, y_pred_prob)  # í™•ë¥ ê°’ ì‚¬ìš©
        score = (f1 + auroc) / 2

        results.append(
            {"file": submission_path.name, "f1": f1, "auroc": auroc, "score": score}
        )

        print(
            f"{submission_path.name:50s} | F1: {f1:.6f} | AUROC: {auroc:.6f} | Score: {score:.6f}"
        )

    except Exception as e:
        print(f"ì˜¤ë¥˜: {submission_path.name} í‰ê°€ ì‹¤íŒ¨ - {e}")

print("\n" + "=" * 80)
print("í‰ê°€ ê²°ê³¼ ìš”ì•½")
print("=" * 80)

if results:
    # ìŠ¤ì½”ì–´ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
    results_sorted = sorted(results, key=lambda x: x["score"], reverse=True)

    print(f"\nì´ {len(results_sorted)}ê°œ íŒŒì¼ í‰ê°€ ì™„ë£Œ\n")
    print("ìƒìœ„ 5ê°œ ê²°ê³¼:")
    print("-" * 80)
    for i, result in enumerate(results_sorted[:5], 1):
        print(f"{i}. {result['file']:50s}")
        print(
            f"   F1: {result['f1']:.6f} | AUROC: {result['auroc']:.6f} | Score: {result['score']:.6f}"
        )
        print()

    # ìµœê³  ìŠ¤ì½”ì–´ íŒŒì¼
    best = results_sorted[0]
    print("=" * 80)
    print("ğŸ† ê°€ì¥ ë†’ì€ ìŠ¤ì½”ì–´ë¥¼ ê°€ì§„ íŒŒì¼")
    print("=" * 80)
    print(f"íŒŒì¼ëª…: {best['file']}")
    print(f"F1 Score: {best['f1']:.6f}")
    print(f"AUROC: {best['auroc']:.6f}")
    print(f"Final Score: {best['score']:.6f}")
    print(f"ì „ì²´ ê²½ë¡œ: {submissions_dir / best['file']}")
    print("=" * 80)
else:
    print("í‰ê°€ëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
