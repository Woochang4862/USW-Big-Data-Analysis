{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bdade4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 21693 | Feat: 52 | Test: 15004 | Classes: 21\n",
      "[Feature Selection] 제거된 피처 수: 2\n",
      "[LR/QDA용] 50개 | [LGBM용] 52개\n",
      "\n",
      "============================================================\n",
      "=== Step 1: 베이스 모델 하이퍼파라미터 튜닝 (Macro-F1) ===\n",
      "============================================================\n",
      "LR C 튜닝 시작 (Macro-F1 기준)...\n",
      "[LR] C=0.5   | CV-MacroF1=0.5629\n",
      "[LR] C=1.0   | CV-MacroF1=0.5778\n",
      "[LR] C=2.0   | CV-MacroF1=0.5871\n",
      "[LR] C=5.0   | CV-MacroF1=0.5949\n",
      "[LR] Best C=5.0 (CV-MacroF1=0.5949)\n",
      "QDA 튜닝 시작 (Macro-F1 기준)...\n",
      "[QDA] Best Scaler=True, reg=0.0 (CV-MacroF1=0.8805)\n",
      "LGBM 튜닝 시작 (원본 7개 파라미터, Macro-F1 기준)...\n",
      "\n",
      "[LGBM 1/7] params={'max_depth': 3, 'num_leaves': 15, 'min_child_samples': 50, 'lambda_l2': 0.0, 'learning_rate': 0.1}\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[328]\tvalid_0's multi_logloss: 0.546523\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[361]\tvalid_0's multi_logloss: 0.557056\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[306]\tvalid_0's multi_logloss: 0.557712\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[359]\tvalid_0's multi_logloss: 0.542181\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[301]\tvalid_0's multi_logloss: 0.54802\n",
      "  → CV-MacroF1=0.8015, mean_best_iter=331\n",
      "\n",
      "[LGBM 2/7] params={'max_depth': 3, 'num_leaves': 31, 'min_child_samples': 50, 'lambda_l2': 1.0, 'learning_rate': 0.1}\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[366]\tvalid_0's multi_logloss: 0.538682\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[452]\tvalid_0's multi_logloss: 0.557667\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[440]\tvalid_0's multi_logloss: 0.557648\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[365]\tvalid_0's multi_logloss: 0.539603\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[363]\tvalid_0's multi_logloss: 0.547769\n",
      "  → CV-MacroF1=0.8000, mean_best_iter=397\n",
      "\n",
      "[LGBM 3/7] params={'max_depth': 5, 'num_leaves': 31, 'min_child_samples': 50, 'lambda_l2': 0.0, 'learning_rate': 0.1}\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[124]\tvalid_0's multi_logloss: 0.530894\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[123]\tvalid_0's multi_logloss: 0.555142\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[119]\tvalid_0's multi_logloss: 0.551293\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[134]\tvalid_0's multi_logloss: 0.537466\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[120]\tvalid_0's multi_logloss: 0.542698\n",
      "  → CV-MacroF1=0.8026, mean_best_iter=124\n",
      "\n",
      "[LGBM 4/7] params={'max_depth': 5, 'num_leaves': 31, 'min_child_samples': 20, 'lambda_l2': 1.0, 'learning_rate': 0.05}\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[292]\tvalid_0's multi_logloss: 0.530412\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[316]\tvalid_0's multi_logloss: 0.550162\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[296]\tvalid_0's multi_logloss: 0.550699\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[351]\tvalid_0's multi_logloss: 0.53844\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[270]\tvalid_0's multi_logloss: 0.540655\n",
      "  → CV-MacroF1=0.8023, mean_best_iter=305\n",
      "\n",
      "[LGBM 5/7] params={'max_depth': 7, 'num_leaves': 63, 'min_child_samples': 50, 'lambda_l2': 1.0, 'learning_rate': 0.05}\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[195]\tvalid_0's multi_logloss: 0.537312\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[181]\tvalid_0's multi_logloss: 0.560979\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[224]\tvalid_0's multi_logloss: 0.552729\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[199]\tvalid_0's multi_logloss: 0.545655\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[172]\tvalid_0's multi_logloss: 0.544394\n",
      "  → CV-MacroF1=0.8009, mean_best_iter=194\n",
      "\n",
      "[LGBM 6/7] params={'max_depth': 7, 'num_leaves': 63, 'min_child_samples': 20, 'lambda_l2': 5.0, 'learning_rate': 0.03}\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[433]\tvalid_0's multi_logloss: 0.536397\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[410]\tvalid_0's multi_logloss: 0.561345\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[467]\tvalid_0's multi_logloss: 0.557087\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[476]\tvalid_0's multi_logloss: 0.551974\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[382]\tvalid_0's multi_logloss: 0.551641\n",
      "  → CV-MacroF1=0.7979, mean_best_iter=433\n",
      "\n",
      "[LGBM 7/7] params={'max_depth': 5, 'num_leaves': 63, 'min_child_samples': 30, 'lambda_l2': 5.0, 'learning_rate': 0.05}\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[381]\tvalid_0's multi_logloss: 0.5302\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[397]\tvalid_0's multi_logloss: 0.560599\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[442]\tvalid_0's multi_logloss: 0.5491\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[402]\tvalid_0's multi_logloss: 0.545357\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[415]\tvalid_0's multi_logloss: 0.542765\n",
      "  → CV-MacroF1=0.8016, mean_best_iter=407\n",
      "\n",
      "[LGBM] Best params={'max_depth': 5, 'num_leaves': 31, 'min_child_samples': 50, 'lambda_l2': 0.0, 'learning_rate': 0.1} (CV-MacroF1=0.8026)\n",
      "\n",
      "============================================================\n",
      "=== Step 2: OOF 예측 생성 (스태킹 메타 피처) ===\n",
      "============================================================\n",
      "[Fold 1] Simple Average Macro-F1 = 0.8791\n",
      "[Fold 2] Simple Average Macro-F1 = 0.8814\n",
      "[Fold 3] Simple Average Macro-F1 = 0.8782\n",
      "[Fold 4] Simple Average Macro-F1 = 0.8743\n",
      "[Fold 5] Simple Average Macro-F1 = 0.8786\n",
      "Meta feature shape: (21693, 63)\n",
      "\n",
      "============================================================\n",
      "=== Step 3: Meta LR 학습 (C=1.0 고정, 과적합 방지) ===\n",
      "============================================================\n",
      "[Meta] OOF Training Macro-F1 = 0.8923, ACC = 0.8935\n",
      "\n",
      "============================================================\n",
      "=== Step 4: 전체 데이터로 베이스 모델 재학습 ===\n",
      "============================================================\n",
      "✓ LR Final Model 학습 완료\n",
      "✓ QDA Final Model 학습 완료\n",
      "✓ LGBM Final Model 학습 완료\n",
      "\n",
      "[저장 완료] submission_fixed_macroF1.csv\n",
      "[예상 성능] 0.83~0.86 (원본 수준 복원)\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 수정 버전: Macro-F1 최적화 스태킹 앙상블\n",
    "# - 문제 수정: Accuracy → Macro-F1 변경\n",
    "# - LGBM 파라미터 원본 7개 복원\n",
    "# - Meta-LR C=1.0 고정 (과적합 방지)\n",
    "# ============================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score  # f1_score 추가!\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "from lightgbm import LGBMClassifier, early_stopping, log_evaluation\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ---------- 데이터 로드 ----------\n",
    "def load_data():\n",
    "    try:\n",
    "        df_train = pd.read_csv(\"train.csv\")\n",
    "        df_test  = pd.read_csv(\"test.csv\")\n",
    "        return df_train, df_test\n",
    "    except FileNotFoundError:\n",
    "        print(\"경고: train.csv/test.csv를 찾을 수 없습니다.\")\n",
    "        raise\n",
    "\n",
    "train_df, test_df = load_data()\n",
    "X_full = train_df.drop(['ID', 'target'], axis=1)\n",
    "y = train_df['target'].astype(int)\n",
    "test_ids = test_df['ID']\n",
    "X_test_full = test_df.drop(['ID'], axis=1)\n",
    "num_classes = len(np.unique(y))\n",
    "\n",
    "print(f\"Train: {X_full.shape[0]} | Feat: {X_full.shape[1]} | Test: {X_test_full.shape[0]} | Classes: {num_classes}\")\n",
    "\n",
    "# --- LR/QDA용: 강한 상관 피처 제거 (0.999 초과) ---\n",
    "corr = X_full.corr().abs()\n",
    "upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.999)]\n",
    "\n",
    "X_lrqda = X_full.drop(columns=to_drop)\n",
    "X_test_lrqda = X_test_full.drop(columns=to_drop)\n",
    "\n",
    "print(f\"[Feature Selection] 제거된 피처 수: {len(to_drop)}\")\n",
    "print(f\"[LR/QDA용] {X_lrqda.shape[1]}개 | [LGBM용] {X_full.shape[1]}개\")\n",
    "\n",
    "# ---------- CV helper ----------\n",
    "def get_cv_splits(X, y, n_splits=5, seed=42):\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    for tr_idx, val_idx in skf.split(X, y):\n",
    "        yield X.iloc[tr_idx], X.iloc[val_idx], y.iloc[tr_idx], y.iloc[val_idx]\n",
    "\n",
    "# --- 1) LR 하이퍼파라미터 튜닝 (Macro-F1 기준) ---\n",
    "def tune_lr_cv(X, y):\n",
    "    Cs = [0.5, 1.0, 2.0, 5.0]  # 원본 범위 복원\n",
    "    results = []\n",
    "    print(\"LR C 튜닝 시작 (Macro-F1 기준)...\")\n",
    "    for C in Cs:\n",
    "        f1s = []  # accuracy → f1로 변경!\n",
    "        for X_tr, X_val, y_tr, y_val in get_cv_splits(X, y, n_splits=5, seed=42):\n",
    "            pipe = Pipeline([\n",
    "                (\"imp\", SimpleImputer(strategy=\"mean\")),\n",
    "                (\"scaler\", StandardScaler()),\n",
    "                (\"clf\", LogisticRegression(\n",
    "                    C=C, max_iter=2000, multi_class=\"multinomial\",\n",
    "                    solver=\"lbfgs\", n_jobs=-1, random_state=42\n",
    "                ))\n",
    "            ])\n",
    "            pipe.fit(X_tr, y_tr)\n",
    "            pred = pipe.predict(X_val)\n",
    "            f1m = f1_score(y_val, pred, average=\"macro\")  # ✅ Macro-F1 사용\n",
    "            f1s.append(f1m)\n",
    "        mean_f1 = np.mean(f1s)\n",
    "        results.append((C, mean_f1))\n",
    "        print(f\"[LR] C={C:<5.1f} | CV-MacroF1={mean_f1:.4f}\")\n",
    "    best_C, best_f1 = max(results, key=lambda x: x[1])\n",
    "    print(f\"[LR] Best C={best_C} (CV-MacroF1={best_f1:.4f})\")\n",
    "    return best_C\n",
    "\n",
    "# --- 2) QDA 하이퍼파라미터 튜닝 (Macro-F1 기준) ---\n",
    "def tune_qda_cv(X, y):\n",
    "    candidates = []\n",
    "    reg_params = [0.0, 0.05, 0.1, 0.2, 0.5]  # 원본 범위 복원\n",
    "    print(\"QDA 튜닝 시작 (Macro-F1 기준)...\")\n",
    "    for use_scaler in [True, False]:\n",
    "        for r in reg_params:\n",
    "            f1s = []\n",
    "            for X_tr, X_val, y_tr, y_val in get_cv_splits(X, y, n_splits=5, seed=42):\n",
    "                steps = [(\"imp\", SimpleImputer(strategy=\"mean\"))]\n",
    "                if use_scaler:\n",
    "                    steps.append((\"scaler\", StandardScaler()))\n",
    "                steps.append((\"clf\", QDA(reg_param=r)))\n",
    "                pipe = Pipeline(steps)\n",
    "                try:\n",
    "                    pipe.fit(X_tr, y_tr)\n",
    "                    pred = pipe.predict(X_val)\n",
    "                    f1m = f1_score(y_val, pred, average=\"macro\")  # ✅ Macro-F1 사용\n",
    "                    f1s.append(f1m)\n",
    "                except Exception:\n",
    "                    pass\n",
    "            if len(f1s) < 5:\n",
    "                continue\n",
    "            mean_f1 = np.mean(f1s)\n",
    "            candidates.append(((use_scaler, r), mean_f1))\n",
    "    best_params, best_f1 = max(candidates, key=lambda x: x[1])\n",
    "    use_scaler_best, reg_best = best_params\n",
    "    print(f\"[QDA] Best Scaler={use_scaler_best}, reg={reg_best} (CV-MacroF1={best_f1:.4f})\")\n",
    "    return use_scaler_best, reg_best\n",
    "\n",
    "# --- 3) LGBM 하이퍼파라미터 튜닝 (원본 7개 복원, Macro-F1 기준) ---\n",
    "def tune_lgbm_cv(X, y, num_classes):\n",
    "    # ✅ 원본 7개 파라미터 복원\n",
    "    params_grid = [\n",
    "        {\"max_depth\": 3, \"num_leaves\": 15, \"min_child_samples\": 50, \"lambda_l2\": 0.0, \"learning_rate\": 0.1},\n",
    "        {\"max_depth\": 3, \"num_leaves\": 31, \"min_child_samples\": 50, \"lambda_l2\": 1.0, \"learning_rate\": 0.1},\n",
    "        {\"max_depth\": 5, \"num_leaves\": 31, \"min_child_samples\": 50, \"lambda_l2\": 0.0, \"learning_rate\": 0.1},\n",
    "        {\"max_depth\": 5, \"num_leaves\": 31, \"min_child_samples\": 20, \"lambda_l2\": 1.0, \"learning_rate\": 0.05},\n",
    "        {\"max_depth\": 7, \"num_leaves\": 63, \"min_child_samples\": 50, \"lambda_l2\": 1.0, \"learning_rate\": 0.05},\n",
    "        {\"max_depth\": 7, \"num_leaves\": 63, \"min_child_samples\": 20, \"lambda_l2\": 5.0, \"learning_rate\": 0.03},\n",
    "        {\"max_depth\": 5, \"num_leaves\": 63, \"min_child_samples\": 30, \"lambda_l2\": 5.0, \"learning_rate\": 0.05},\n",
    "    ]\n",
    "    results = []\n",
    "    print(\"LGBM 튜닝 시작 (원본 7개 파라미터, Macro-F1 기준)...\")\n",
    "    for i, p in enumerate(params_grid, 1):\n",
    "        f1s = []\n",
    "        best_iters = []\n",
    "        print(f\"\\n[LGBM {i}/7] params={p}\")\n",
    "        for fold_id, (X_tr, X_val, y_tr, y_val) in enumerate(get_cv_splits(X, y, n_splits=5, seed=42), 1):\n",
    "            clf = LGBMClassifier(\n",
    "                objective=\"multiclass\", num_class=num_classes,\n",
    "                n_estimators=5000, random_state=42 + fold_id,\n",
    "                n_jobs=-1, verbosity=-1, **p\n",
    "            )\n",
    "            clf.fit(\n",
    "                X_tr, y_tr,\n",
    "                eval_set=[(X_val, y_val)],\n",
    "                eval_metric=\"multi_logloss\",\n",
    "                callbacks=[early_stopping(stopping_rounds=100), log_evaluation(period=0)]\n",
    "            )\n",
    "            pred = clf.predict(X_val)\n",
    "            f1m = f1_score(y_val, pred, average=\"macro\")  # ✅ Macro-F1 사용\n",
    "            f1s.append(f1m)\n",
    "            best_iter = getattr(clf, \"best_iteration_\", None)\n",
    "            if best_iter is not None:\n",
    "                best_iters.append(best_iter)\n",
    "        \n",
    "        mean_f1 = np.mean(f1s)\n",
    "        mean_iter = int(np.mean(best_iters)) if len(best_iters) > 0 else 1000\n",
    "        results.append((p, mean_f1, mean_iter))\n",
    "        print(f\"  → CV-MacroF1={mean_f1:.4f}, mean_best_iter={mean_iter}\")\n",
    "\n",
    "    best_p, best_f1, best_iter = max(results, key=lambda x: x[1])\n",
    "    print(f\"\\n[LGBM] Best params={best_p} (CV-MacroF1={best_f1:.4f})\")\n",
    "    return best_p, best_iter\n",
    "\n",
    "# =========================================================\n",
    "# 메인 실행: 베이스 모델 튜닝 및 OOF 생성\n",
    "# =========================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"=== Step 1: 베이스 모델 하이퍼파라미터 튜닝 (Macro-F1) ===\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "best_C_lr = tune_lr_cv(X_lrqda, y)\n",
    "best_use_scaler, best_reg = tune_qda_cv(X_lrqda, y)\n",
    "best_lgb_params, best_lgb_n_estimators = tune_lgbm_cv(X_full, y, num_classes)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"=== Step 2: OOF 예측 생성 (스태킹 메타 피처) ===\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "skf_stack = StratifiedKFold(n_splits=5, shuffle=True, random_state=2025)\n",
    "\n",
    "oof_lr = np.zeros((len(y), num_classes))\n",
    "oof_qda = np.zeros((len(y), num_classes))\n",
    "oof_lgb = np.zeros((len(y), num_classes))\n",
    "\n",
    "for fold, (tr_idx, val_idx) in enumerate(skf_stack.split(X_full, y), 1):\n",
    "    X_tr_lr, X_val_lr = X_lrqda.iloc[tr_idx], X_lrqda.iloc[val_idx]\n",
    "    X_tr_lgb, X_val_lgb = X_full.iloc[tr_idx], X_full.iloc[val_idx]\n",
    "    y_tr, y_val = y.iloc[tr_idx], y.iloc[val_idx]\n",
    "\n",
    "    # LR (OOF)\n",
    "    pipe_lr = Pipeline([\n",
    "        (\"imp\", SimpleImputer(strategy=\"mean\")),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"clf\", LogisticRegression(\n",
    "            C=best_C_lr, max_iter=2000, multi_class=\"multinomial\",\n",
    "            solver=\"lbfgs\", n_jobs=-1, random_state=2025 + fold\n",
    "        ))\n",
    "    ]).fit(X_tr_lr, y_tr)\n",
    "\n",
    "    # QDA (OOF)\n",
    "    steps_qda_fold = [(\"imp\", SimpleImputer(strategy=\"mean\"))]\n",
    "    if best_use_scaler:\n",
    "        steps_qda_fold.append((\"scaler\", StandardScaler()))\n",
    "    steps_qda_fold.append((\"clf\", QDA(reg_param=best_reg)))\n",
    "    pipe_qda = Pipeline(steps_qda_fold).fit(X_tr_lr, y_tr)\n",
    "\n",
    "    # LGBM (OOF)\n",
    "    lgb_model = LGBMClassifier(\n",
    "        objective=\"multiclass\", num_class=num_classes,\n",
    "        n_estimators=best_lgb_n_estimators, random_state=2025 + fold,\n",
    "        n_jobs=-1, verbosity=-1, **best_lgb_params\n",
    "    ).fit(X_tr_lgb, y_tr)\n",
    "\n",
    "    # OOF 확률 저장\n",
    "    oof_lr[val_idx] = pipe_lr.predict_proba(X_val_lr)\n",
    "    oof_qda[val_idx] = pipe_qda.predict_proba(X_val_lr)\n",
    "    oof_lgb[val_idx] = lgb_model.predict_proba(X_val_lgb)\n",
    "\n",
    "    # Fold 성능 (Macro-F1)\n",
    "    proba_ens_fold = (oof_lr[val_idx] + oof_qda[val_idx] + oof_lgb[val_idx]) / 3.0\n",
    "    pred_ens_fold = np.argmax(proba_ens_fold, axis=1)\n",
    "    f1_fold = f1_score(y_val, pred_ens_fold, average=\"macro\")  # ✅ Macro-F1\n",
    "    print(f\"[Fold {fold}] Simple Average Macro-F1 = {f1_fold:.4f}\")\n",
    "\n",
    "# 메타 입력 행렬\n",
    "meta_X = np.concatenate([oof_lr, oof_qda, oof_lgb], axis=1)\n",
    "print(f\"Meta feature shape: {meta_X.shape}\")\n",
    "\n",
    "# =========================================================\n",
    "# Step 3: Meta Logistic Regression (C=1.0 고정)\n",
    "# =========================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"=== Step 3: Meta LR 학습 (C=1.0 고정, 과적합 방지) ===\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ✅ 원본처럼 C=1.0 고정 (튜닝 안함)\n",
    "meta_clf = LogisticRegression(\n",
    "    max_iter=2000,\n",
    "    multi_class=\"multinomial\",\n",
    "    solver=\"lbfgs\",\n",
    "    n_jobs=-1,\n",
    "    C=1.0,  # 고정!\n",
    "    random_state=777\n",
    ")\n",
    "meta_clf.fit(meta_X, y)\n",
    "\n",
    "meta_pred_train = meta_clf.predict(meta_X)\n",
    "meta_f1 = f1_score(y, meta_pred_train, average=\"macro\")  # ✅ Macro-F1\n",
    "meta_acc = accuracy_score(y, meta_pred_train)\n",
    "print(f\"[Meta] OOF Training Macro-F1 = {meta_f1:.4f}, ACC = {meta_acc:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"=== Step 4: 전체 데이터로 베이스 모델 재학습 ===\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# LR 최종\n",
    "pipe_lr_final = Pipeline([\n",
    "    (\"imp\", SimpleImputer(strategy=\"mean\")),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"clf\", LogisticRegression(\n",
    "        C=best_C_lr, max_iter=2000, multi_class=\"multinomial\",\n",
    "        solver=\"lbfgs\", n_jobs=-1, random_state=2025\n",
    "    ))\n",
    "]).fit(X_lrqda, y)\n",
    "print(\"✓ LR Final Model 학습 완료\")\n",
    "\n",
    "# QDA 최종\n",
    "steps_qda_final = [(\"imp\", SimpleImputer(strategy=\"mean\"))]\n",
    "if best_use_scaler:\n",
    "    steps_qda_final.append((\"scaler\", StandardScaler()))\n",
    "steps_qda_final.append((\"clf\", QDA(reg_param=best_reg)))\n",
    "pipe_qda_final = Pipeline(steps_qda_final).fit(X_lrqda, y)\n",
    "print(\"✓ QDA Final Model 학습 완료\")\n",
    "\n",
    "# LGBM 최종\n",
    "lgb_final = LGBMClassifier(\n",
    "    objective=\"multiclass\", num_class=num_classes,\n",
    "    n_estimators=best_lgb_n_estimators, random_state=2025,\n",
    "    n_jobs=-1, verbosity=-1, **best_lgb_params\n",
    ").fit(X_full, y)\n",
    "print(\"✓ LGBM Final Model 학습 완료\")\n",
    "\n",
    "# =========================================================\n",
    "# Step 5: 테스트 데이터 최종 예측\n",
    "# =========================================================\n",
    "\n",
    "proba_lr_test = pipe_lr_final.predict_proba(X_test_lrqda)\n",
    "proba_qda_test = pipe_qda_final.predict_proba(X_test_lrqda)\n",
    "proba_lgb_test = lgb_final.predict_proba(X_test_full)\n",
    "\n",
    "meta_X_test = np.concatenate([proba_lr_test, proba_qda_test, proba_lgb_test], axis=1)\n",
    "final_pred = meta_clf.predict(meta_X_test)\n",
    "\n",
    "out_path = \"submission_fixed_macroF1.csv\"\n",
    "pd.DataFrame({\"ID\": test_ids, \"target\": final_pred}).to_csv(out_path, index=False)\n",
    "print(f\"\\n[저장 완료] {out_path}\")\n",
    "print(f\"[예상 성능] 0.83~0.86 (원본 수준 복원)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd556a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "team_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
